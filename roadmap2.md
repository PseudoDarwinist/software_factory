Ok i am Planning to create a Software factory APP.


our Product Stack is a Cry for Help

Whether you’re a solo builder or a team, collectively you are the entire industrial assembly line.

Your current stack?

- Notion docs
- Figma screenshots
- GitHub issues with no assignees
- Slack messages like “is this still broken?”
- ChatGPT hallucinations
- A whiteboard you haven’t erased since May

It's less of a workflow and more of a coping mechanism.

Software Factory turns all that chaos into one product system:

Think →  PRD → Plan →  Build →  Validate

It's ideal for founders and product/dev leaders with too many tabs and not enough hands.

Software Factory starts lean but when you scale, we scale with you.

AI writing code is helpful but incomplete. Try shipping something from an app-crapper that doesn't just fall apart at the seams when someone changes one button.

Product teams need a product OS. 

A space where Business people, PMs, Designers, Eng Managers, Developers and QA can each play their role in shipping hi quality product to production.


while AI can help you write code... it still doesn’t do a great job of helping you build a product.

Software Factory is not a chatbot. It's your productOS, syncing PRD → Specs → Code → QA.

Software Factory is built for people who want mission control, not tool sprawl and drift.

Think code-gen is enough? That’s just one part of a successful product org. 


Cursor, Replit etc work exceptionally well for engineers writing code, but what about all the other personas involved in building products?

Product Managers? Engineering Managers? QA?

This is why we built Software Factory to help us ship production code more consistently, with higher quality and better accountability. 

It works for us and may be useful to you...

1. Software Factory automates key components of the SDLC into one simple workflow. It obviates the need for the endless sprawl of various tools that all these personas use today to ship product.

2. Software Factory also orchestrates workflow for all of the people involved in bringing great products to life. There is now a place for every Product manager, Engineering manager, developer and quality assurance person to play their role.

3. Software Factory unifies these workflows in a shared space- your Software Factory - and lets you ship a durable product quickly so you don't miss your opportunity.

4. Great products evolve, but evolving great products is hard. The knowledge graph inside Software Factory makes evolving your product effortless. Five new people could onboard tomorrow and start working on features with no warm-up.

Software Factory is the tool to make all experts 10x more effective.



The Bitter Lesson

Definition: General methods of problem solving that leverage computation ultimately win out over narrow methods that try to encode human knowledge or biases.

The bitter lesson is playing out in realtime right now in autonomy (Tesla vs Waymo) and in Foundational AI model building (Grok v OpenAI v Gemini v Llama) with Trillions of dollars of economic value on the line in each case. 

So far, betting on computation has been the winning approach while betting on narrow but deep knowledge acquisition/labelling is coming in second. 

If the bitter lesson can be observed in autonomy and AI models, where may we see it play out next?

Our bet is in product development. 

What We want to do with Software Factory is learning from the bitter lesson and applying it to product development. 

Current agentic systems are overfit on writing React components, producing plastic software that looks okay and may make you feel good but ultimately never ships. 

So what’s the point?

our Software Factory applies intelligence generally across the SDLC to build durable, maintainable products instead of flimsy demos.

Additionally, the knowledge graphs and intermediate representations produced are some of the most valuable in the world. Long-horizon reasoning across domains with tool calls AND verifiable rewards. We think it’s one of the holy grails of RL. 


Have you ever wondered why all companies, independent of size or sector, all seem to have the same org chart?

The titles repeat. The hierarchies repeat. The roles and responsibilities repeat...just the acronyms and vernacular from industry to industry seem to change.

Is this really ideal? 

If so, then why do profit margins and ROIC differ so much across companies and across industries? Why wouldn't a poor performing company experiment with a different organizational model to improve obviously poor performance? On the other hand, why do some companies buck these trends and usually massively overperform financially vs their peer competitors?

My hypothesis is that the modern organizational design of for-profit companies is a direct artifact of how the Software Industrial Complex ("SIC") has mapped out work and then sold in Trillions of dollars of software to automate and semi-automate that work. But instead of giving a company organizational flexibility, it creates brittle lock in with roles and job descriptions that become increasingly more rigid and inflexible. 

Meanwhile, overperforming companies usually have one thing in common - they limit the amount of boxed software from the SIC and instead roll their own. The byproduct tends to be radically different organizational models that often don't make sense to outsiders.

Behind every title in an org chart of a legacy company, you can likely find some system of record or SaaS tool from the SIC that was sold in. The org design was an output of the software vs the other way around.

In a world of AI, I think we can radically reimagine org design. And, as a byproduct, lift productivity and profitability.  When Agents and smart models with low error rates do work, the org chart can look very different...and become very streamlined.

You can have new processes that simply cut across functions to get the work that needs to be done....done. But this requires a lot of new, custom software.

Thats why we want to build Software Factory. To help companies large and small tackle this problem.

Our Software Factory is giving many companies a way of replacing a lot of their dependency in the Software Industrial Complex with custom, hi-quality production software. It helps them make software that gets work done...done.


Software Factory allows teams from solo PM-Developers up to large groups to organize themselves, write quality requirements, build thorough Eng plans, extract detailed tickets, write quality code, QA thoroughly and repeat this process in an increasingly automated way until the project works. It is tuned for complex environments, making it also work very elegantly for de novo projects. 

Based on the details of Software Factory App , how would you build this step by step. Think Deep, really really deep.Go Ultra-deep thinking mode. Greater rigor, attention to detail, and multi-angle verification. 

Please speak in plain, jargon-free, tables(Dnt draw tables) free english please.

I need the conceptualisation of idea into concrete implementable App.STep by Step what should the UI/UX look like which will sove every problem that i mentioned.

Below is a concrete, end-to-end plan for turning “Software Factory” from an idea into a working product. I break it into five layers—Vision, Workflow, System Design, UI/UX, and Delivery—so you can move from concept to code without getting lost. Every step is expressed in plain English, with no jargon or tables.

⸻

1  Vision layer – agree on the “why”
	1.	Articulate the single promise
“One place that turns messy ideas into production-ready software, while every contributor sees exactly what to do next.”
	2.	Name the personas and the win for each
	•	Founder / Exec – see progress and risk in one glance.
	•	Product Manager – write a PRD once; never chase status again.
	•	Designer – push designs; get instant spec diffs.
	•	Engineer – fewer tabs; context follows the code.
	•	QA – tests auto-appear where new behaviour appears.
	3.	Define the success metric
“Idea-to-deployment lead time falls by 50 % within three months of adoption.”

⸻

2  Workflow layer – map Think → PRD → Plan → Build → Validate
	1.	Think
	•	Capture any idea (Slack message, whiteboard snapshot, figma link).
	•	Auto-tag by topic and urgency.
	2.	PRD
	•	Convert the chosen idea into a structured doc: problem, outcome, guard-rails.
	•	AI suggests missing sections and detects conflicting requirements.
	3.	Plan
	•	Break PRD into milestones and tickets in one click.
	•	Auto-assign owners from past contribution history.
	4.	Build
	•	Branch is created with the ticket ID; commit messages link back to PRD section.
	•	Copilot panel surfaces relevant code snippets and design specs inline.
	5.	Validate
	•	When a pull request opens, QA checklist is auto-generated from spec changes.
	•	CI pipeline posts results back into the same thread; green build merges after two approvals.

Each hand-off writes nodes and edges into one shared knowledge graph, so nothing is lost when people change or scale.

⸻

3  System-design layer – what runs under the hood
	1.	Core storage – Graph database (e.g. Neo4j) for artifacts + Postgres for transactional data.
	2.	Event bus – Kafka or NATS to broadcast “node added / edge added” events to all services.
	3.	Service slices
	•	Capture (ingests Slack, Notion, Figma, Git, etc.).
	•	Graph (creates and queries relationships).
	•	AI (LLM wrappers, RAG against graph, evaluation feedback).
	•	Auth & Roles (JWT + role matrix).
	•	Realtime Collab (CRDTs or Y.js over WebSocket).
	4.	Extensibility – plugin framework where any internal team can post a new event listener (e.g. “when label=security, run SAST scan”).
	5.	Data consistency – Source of truth is the graph ID, not the tool link, so renames or moves never break references.

⸻

4  UI / UX layer – what users touch

4.1  Overall navigation
	•	Left rail: Workspaces in the Think → Validate order.
	•	Top bar: Global search, command palette, profile.
	•	Right panel: Contextual AI assistant—always shows “here’s what matters next”.

4.2  Key screens
	1.	Mission Control (Home)
	•	A vertical timeline of “blocking events” across all products, coloured by risk.
	•	Quick-filter chips: “my tasks”, “needs review”, “tests failing”.
	2.	Idea Inbox (Think)
	•	Infinite scroll of raw notes with AI-generated tags.
	•	One-tap triage: keep, merge, archive.
	3.	PRD Editor
	•	Rich-text sections with status badges (“complete”, “needs numbers”).
	•	Side-by-side diff view when a spec changes.
	4.	Planner
	•	Hybrid roadmap + kanban.
	•	Dragging a card to “in progress” spawns a branch and checklist.
	5.	Code View
	•	Embedded Git viewer; commits grouped by ticket.
	•	“Spec trace” button highlights which PRD line each diff fulfils.
	6.	Test Lab (Validate)
	•	Grid of test cases vs latest runs.
	•	Hover reveals repro steps written by AI from real usage traces.
	7.	Knowledge Graph Explorer
	•	Force-directed map you can click through (Idea → PRD → Ticket → Commit → Test).
	•	Search bar accepts natural language (“show me all features touching payments this quarter”).

4.3  Interaction patterns that cut noise
	•	Command palette everywhere ( ⌘K ): “assign to Alex”, “generate test plan”.
	•	Live presence cursors so each role sees who else is editing.
	•	Undo stack stored per artifact, so experimentation is safe.
	•	Dark-mode first to lessen glare in long work sessions.

⸻

5  Delivery layer – how to build it without drowning

Phase 0 : Proof-of-concept (2–3 weeks)
	•	Build the graph service with one node type (Idea) and one edge (refines-into-PRD).
	•	Hard-code Slack capture and a simple PRD editor.
	•	Prove the end-to-end flow: message → PRD → graph.

Phase 1 : Alpha (8 weeks)
	•	Add the Plan workspace, ticket extraction, Git branching.
	•	Roll out to an internal team shipping a real minor feature.
	•	Collect friction points daily; adjust.

Phase 2 : Beta (12 weeks)
	•	Ship Validate workspace with CI hooks and test-plan generator.
	•	Integrate Figma snapshots and commit-to-design diffing.
	•	Invite three external design-partner companies.

Phase 3 : General Availability (ongoing)
	•	Harden auth, RBAC, rate limiting.
	•	Launch plugin SDK so power users connect legacy tools.
	•	Publish SLA and pricing; scale infra behind a feature flag system.

Guard-rails the whole way
	•	Instrument everything (time from idea to prod, error rates, abandoned tickets).
	•	Shadow test new AI features off the mainline before auto-applying output.
	•	Manual override everywhere: a human can freeze or roll back any artifact.

⸻

Final checkpoints before you write the first line of code
	1.	Can every persona answer “What do I owe the team today?” in one click?
	2.	Does an artifact ever live in only one place? It should not.
	3.	Is every AI suggestion traceable and reversible? Yes, via the graph.

If those three are true, you have an app that really turns chaos into a factory—and a clear, step-by-step roadmap to build it.

Imagine your product team wakes up on Monday and decides that the application needs a brand-new “Dark Mode.” I will follow that single idea from the first chat message all the way to production and, on the way, show you exactly where the graph database, Postgres, and the event bus step in, why they matter, and what real data looks like inside them.

⸻

1  A message appears in Slack — why a graph database is involved

Sara the designer types, “Could we add Dark Mode? Marketing says lots of users ask for it.”

A tiny background service is watching the “#ideas” channel. The moment it sees Sara’s message it does two things:
	1.	It writes an “Idea” node into Neo4j.
Think of Neo4j as a whiteboard covered in sticky notes connected by strings. Every sticky note is a node, every string is a relationship. The service creates

(:Idea {  
  id: "idea-1337",  
  text: "Add Dark Mode",  
  author: "sara",  
  timestamp: 2025-07-14T08:03:00Z  
})

No tables, no foreign keys, just one self-contained object.

	2.	It saves a record into Postgres.
Postgres is your ledger for things that must never get confused, like who owns what workspace, whose subscription paid for the seat, whether Sara is allowed to create ideas, and the full audit log. The service writes:

INSERT INTO user_events
  (user_id, verb, object_id, time)
VALUES
  ('sara', 'created_idea', 'idea-1337', '2025-07-14 08:03:00+00');

That goes into a neat, transactional table where you can always roll back or query with SQL.

Why do we keep two stores?
The graph database is perfect for questions that bounce along relationships: “Show me every pull request that implements an idea that came from Marketing.” Postgres is perfect for questions that need strict accounting and filters: “Give me every idea created last quarter by a designer in the Paris office.” You could do each job with the other tool, but the performance and clarity would be worse.

⸻

2  Telling the rest of the system — how the event bus works

After writing the node and the row, the capture service emits a message to Kafka that literally says, in JSON:

{
  "event": "node_added",
  "node_type": "Idea",
  "node_id": "idea-1337",
  "who": "sara",
  "when": "2025-07-14T08:03:00Z"
}

Think of Kafka as the office PA system. The capture service walks up to the microphone, states “A new idea arrived,” and then walks away. Dozens of little listeners are always tuned in:
	•	The Notification service hears the message, checks the “product-manager” group in Postgres, and sends Lee the PM a ping.
	•	The AI summarizer hears the message, pulls the Slack thread, drafts a one-sentence summary, and writes that back onto the idea node in Neo4j.
	•	The Planner hears the message and queues up, “When this idea becomes a PRD, please create an empty milestone placeholder.”

None of these services talk to each other directly. They only share events. That keeps coupling low: replace Kafka with NATS tomorrow and nothing changes except the plumbing under the hood.

⸻

3  Transforming an idea into a PRD — again Neo4j plus Postgres

Lee opens the web app, clicks “Convert to PRD,” and starts filling out problem statement, acceptance criteria, and metrics.

Every time Lee hits Save, the PRD editor does two writes:
	•	Graph write. It creates a new node

(:PRD {  
  id: "prd-987",  
  title: "Dark Mode",  
  status: "draft",  
  owner: "lee",  
  linked_idea: "idea-1337"  
})

and a relationship

(:Idea {id: "idea-1337"})-[:REFINED_INTO]->(:PRD {id:"prd-987"})


	•	Transactional write. It updates a prd_versions table in Postgres so you have an immutable history:

INSERT INTO prd_versions
  (prd_id, version, json_body, editor, edited_at)
VALUES
  ('prd-987', 1, '{...full PRD JSON...}', 'lee', now());



You now have the same information expressed twice, but for different needs: Neo4j for traversing relationships, Postgres for exact, diff-able history and audit.

Another event, "node_added" for type "PRD", goes onto Kafka. The Planner service, which cares deeply about PRDs, reacts.

⸻

4  Planning work — how the event bus glues new services in

Planner reads the PRD text, calls the AI service: “Break this into milestones and engineering tickets.” The AI replies with three milestones and twelve tickets. Planner writes twelve Ticket nodes in Neo4j, relates them to "prd-987", and then fires "node_added" events for each Ticket.

Meanwhile it also inserts twelve rows in Postgres tickets table so you can filter tickets by sprint, assignee, status with classic SQL.

A "node_added" event for Tickets triggers:
	•	The Repository Manager to create Git branches like feature/dark-mode/header-toggle.
	•	The Notification service to mark each developer’s “Today’s work” list.
	•	The Analytics pipeline to start measuring lead time for this feature.

All of that orchestration happens without Planner knowing or caring who else is around. That is the magic of the event bus.

⸻

5  Writing code — linking Git to the graph

Jamie the engineer commits:

git commit -m "ticket-422: add theme context and toggle"

A small Git hook sees ticket-422 in the message. It calls back to the Build service, which does the familiar two-part dance:
	1.	Write a Commit node in Neo4j with a relationship

(:Ticket {id:"ticket-422"})-[:FULFILLED_BY]->(:Commit {sha:"abc123"})


	2.	Insert a row in Postgres commits for precise diff tracking, author, time, checksum.

Then it publishes "node_added" for type "Commit". That event wakes up:
	•	CI Runner, which queues a build and test run.
	•	Spec Trace service, which compares code changes with PRD lines to highlight unaddressed acceptance criteria.

⸻

6  Validating work — tests speak the same event language

CI finishes. The Test service posts a "test_result" event with "status": "failed" for commit "abc123". Neo4j now gets a TestResult node related to the commit; Postgres gains a test_runs row. The Notification service pushes a message to Jamie and Lee: “Dark Mode header toggle failed scenario X, click to see logs.” Again, no hard wiring — just events.

When the tests go green, a "test_result" event with "status": "passed" triggers the Merge service to automatically merge the pull request because the policy in Postgres says “two approvals plus green tests.” The Merge service writes yet another node, Deployment, attaches it to prd-987, and Kafka echoes the good news to everyone.

⸻

7  Why the whole system needs both stores plus events
	•	Neo4j gives you an ever-growing living map. Anyone can ask, “Show me everything that traces back to Sara’s original Slack post.” The answer is one fast graph traversal, no joins needed.
	•	Postgres gives you guarantees. If two people try to assign the same ticket at the same time, you need a transaction to avoid a mess. If finance audits your ISO-27001 controls, you need every user action immutably stored in a way SQL can slice and dice.
	•	Kafka or NATS keeps every service small and replaceable. Without the bus, your Planner would have to know how to call Notifications, Repos, CI, Analytics, and any future tool you invent. With the bus, it shouts into the crowd and walks away. The right listeners pick up the job.

⸻

8  What if you skipped one of these pieces?

Put everything in Postgres and skip the graph? Querying “find all tickets blocked by designs that changed since Monday” becomes a nightmare of joins and recursive CTEs that slow down as data grows.

Skip Postgres and rely only on the graph? When two people update the same PRD at once you need transactional conflict detection. Neo4j’s ACID support is good but not as mature or familiar as Postgres; you would end up rebuilding functionality Postgres gives you for free.

Skip the event bus and just call each service directly? Every new feature means editing old code in three or four places to wire new dependencies. After a year the system freezes under its own coupling.

⸻

9  Where the junior engineer actually writes code
	•	They write a message consumer that listens to "node_added" for "Commit" and triggers an API call to CI. That is under 100 lines of Python.
	•	They add a Postgres migration file to create ci_runs(id, commit_sha, status, started_at, finished_at).
	•	They add one Neo4j write in the Build service: create a Build node and relate it to the Commit node.
	•	They publish "build_started" and "build_finished" events. Nothing else changes.

Because the pattern is “capture, write to both stores, broadcast event,” any new service follows the same three steps. The system stays understandable even when it grows.

⸻

You now have the whole picture in concrete terms. A single Slack sentence becomes two database entries, an event fan-out, and a chain reaction across half a dozen small services—each doing one thing, each driven by the same rules. With that mental model a junior engineer can add features without guessing why a graph, why SQL, or why an event bus exist.

Let me reset and focus on what you actually need: a picture—told in everyday language—of the product your users will touch and the intelligence that will run it. I will stay on the surface of the screen first, describing the experience a founder, a designer, or a developer will have in front of the app. After that I will show how the autonomous helpers (the “agents”) work behind the scenes. I will mention the data layer only where it explains why an agent can do its job, not because the data layer is the star.

⸻

What a person sees the first minute they open Software Factory

When the browser tab loads the visitor is dropped into one clean, dark-themed hub called Mission Control. The screen is not a dashboard stuffed with charts. It is almost empty except for three vertical zones that read like a story.
	1.	The left column is a narrow rail that simply lists products or projects. A tiny dot next to each name glows red, amber, or green to say whether something needs attention. The dot is generated by an AI policy: it scans every branch, every unanswered question, every failing test, rolls them up, and colours the state. A project that is sailing smoothly earns a green dot and stays quiet.
	2.	The centre column is a live feed of “things that require a human decision.” If a designer just changed the spacing in a core button component the feed shows, “Spacing updated, review visual diffs?” If an automated test failed it shows, “Payment flow broke on Safari, approve rollback?” There is no pagination, no hunt through sub-menus; the feed is the single place you look.
	3.	The right column is the conversational panel. Whatever item is selected in the feed appears here, surrounded by context. When you click on the failed test you see the diff in code, the test runner’s log, the relevant part of the product document, and the Slack thread where the bug was first reported. Under all that is a prompt box that always starts with a suggestion like, “Type ‘/fix’ to ask the Build agent for a patch.” In other words, it feels like chatting with a colleague who already collected every scrap of evidence.

Nothing on this opening screen shows a database table, a ticket number, or a branch name unless you explicitly ask.

⸻

How a new feature idea moves through the interface, step by step

A marketing manager has an idea: “Offer users Dark Mode.” She types that sentence in Slack. An always-on listener in the background, which the team calls the Capture agent, notices any sentence that looks like an improvement request and quietly lifts it into Mission Control. Now the idea shows up as a grey card in the centre feed under the heading “Untriaged thoughts.”

The product manager drags the card into a workspace labelled “Define.” The moment she drops it, the right-hand conversation panel expands into a full-screen editor called the Product Brief. The text she sees is not empty: an agent has already inserted a template with a problem statement, success metrics, risks, and a short competitive analysis pulled from public docs. The PM only has to fill in the blanks or correct the AI’s assumptions. At the bottom of the editor there is a blue button that reads Freeze spec. Clicking it snaps a read-only version into the knowledge graph so that all future work can be compared to it.

The planning view opens automatically. As soon as the spec is frozen the app slides to a lane-style board that has three columns—“Ready,” “In progress,” and “Done.” In the “Ready” column fresh cards appear: “Create theme palette,” “Update header styling,” “Write migration script,” “Add unit tests,” “Run accessibility review.” These cards were written by the Planner agent that read the spec and split the work. Each card already carries a suggested assignee based on past contributions; the assignee can accept or swap by drag-and-drop.

Developers start coding. When a developer picks up “Create theme palette,” the Build agent opens a side drawer that displays the design tokens pulled from Figma plus skeleton code in the language of the repository. The engineer can accept the patch or modify it. Once code is pushed, the Build agent links the commit back to the spec and warns if files outside the intended scope were changed.

Quality assurance kicks in without being asked. The moment a pull request opens, the Test agent generates new test cases. If the change touches colour variables it writes tests that flip the site into Dark Mode and compare pixel deltas. If the agent sees no coverage for low-vision contrast it asks for a manual sign-off. The tester does not start with a blank checklist; it is auto-drafted.

Validation flows back to the brief. When the test suite passes, the system automatically attaches the results and a video replay to the original Product Brief. Anyone who reads the brief a month later will see a living document: idea, spec, code, tests, demo—all stitched together in time order.

Throughout the journey, the person in front of the screen never had to pick which tool to open next. The UI presented only the panel that matched the stage: idea capture, spec writing, planning, building, validating.

⸻

Where the “agentic” behaviour actually comes from

Each agent is a small service with three predictable habits.
	•	It watches the event stream for something it understands—a new idea, a frozen spec, a fresh pull request, a passed or failed test.
	•	It pulls the context it needs from the knowledge graph. That graph is simply a living mind-map that says, “This idea relates to that spec, which links to these tasks, which link to these commits, which link to these tests.”
	•	It calls a large language model with a prompt that includes that context plus a clearly stated goal: “Break the spec into tasks” or “Generate tests for changed colour variables.” The model responds with plain text or code. The agent checks the answer against simple rules (no missing fields, no invalid JSON) and then emits a follow-up event like “tasks_created” or “tests_generated.”

Because every agent follows the same ritual—listen, fetch context, ask the model, emit an event—new agents are easy to add. A compliance agent that checks accessibility guidelines can be written in a week because it only needs to know when a spec freezes and when code changes.

⸻

Why I even mentioned Neo4j, Postgres and Kafka in the first place

You hire agents to remember vast context and to stay in sync. A knowledge graph is the most straightforward way to remember, “Idea A is refined by Spec B which is implemented by Commit C which failed Test D.” A transactional store is the safest way to keep factual logs like who clicked “Merge” and who is allowed to deploy. An event stream is the least fragile way to tell every agent that something new happened. Those three ingredients are not the product; they are the plumbing that lets the agents work reliably. If the names Neo4j, Postgres and Kafka feel abstract, think of them as the bookshelf, the ledger and the loudspeaker. The real show is the group of agents holding a discussion on that loudspeaker.

⸻

Turning this description into the first set of screens you can code tomorrow
	1.	Prototype Mission Control. Build a React page with the three-column layout. Hard-code one project, one feed item, and a right-hand chat box. Wire the chat box to hit OpenAI’s completion API with a fixed prompt like “Summarise this bug.” Do not store anything yet. The goal is to feel the flow between feed and conversation.
	2.	Add the idea-to-brief path. Drop a textarea above the feed so a user can type “Add Dark Mode” and hit Enter. Have the app call the model to expand that to a brief outline and display it in a modal. For the very first prototype save both the raw sentence and the outline in browser memory; persistence can wait.
	3.	Create a mock planning board. When the user clicks “Freeze spec,” run another model call that returns three fake tasks and draw draggable cards. Again, store them only in memory.
	4.	Stub the Build drawer. When a card is dragged to “In progress,” display code suggestions in a panel. The code can be nonsense at first; the experience of accepting or editing suggestions is what matters.
	5.	Stub the Test panel. When the pull request is marked “Open,” auto-fill a test checklist and a green or red badge.

These five steps let you walk a colleague through the full story in under ten minutes. Only after the walkthrough feels smooth should you decide where real data lives and how real agents talk. Databases and event busses will slot in naturally once the front-end demands them.

⸻

The only principle to keep in mind

Never show the user a decision point without surrounding context, and never store data without an agent that can turn that data into a suggestion. If you follow that rule the interface stays calm, the agents stay useful, and the plumbing underneath remains invisible—right where it belongs.
Think of Mission Control as the foyer of a house and of Think → Define → Plan → Build → Validate as the five rooms that branch off that foyer.
Opening the app always lands you in the foyer, because that is where urgent news and “what needs my attention right now?” live. From there you can step into any room to do deeper work on a single idea.

⸻

Walking through one idea again, paying attention to where you are in the house
	1.	First screen – Mission Control (the foyer).
The left rail lists every product (“Kitchen Renovation,” “Dark Mode,” “Payments V2”).
The middle column shows only red-flag or yellow-flag messages that a person must decide on.
The right column opens the supporting evidence for whichever alert you click.
While you are in this foyer you are not in any particular stage; you are simply monitoring the whole house.
	2.	Slack brings in “Offer users Dark Mode.”
The Capture agent hears that sentence and drops a grey card in the middle column under a small header “Untriaged thoughts.”
Nothing else changes on the screen; you are still in Mission Control.
	3.	You pick up the card and drag it to the top bar labelled “Define.”
That single drag is you leaving the foyer and stepping into the “Define” room.
The UI changes to match the room you just entered:
	•	the middle area now shows every grey card in this room (ideas waiting to become real briefs);
	•	the right-hand side stretches out into a larger editor called “Product Brief.”
The overall chrome (logo, account menu, dark background) stays the same, so you never feel like you loaded a separate web site; you only swapped rooms.
	4.	You fill the brief and press “Freeze spec.”
Freezing is the cue to walk into the next room, “Plan.”
The UI slides to a lane board—Ready, In Progress, Done—because boards are the main furniture of the Plan room.
The card “Create theme palette” appears in the Ready lane, already tagged with an owner.
Again you are still inside the same web app; you have just moved one room farther from the foyer.
	5.	A developer drags “Create theme palette” into In Progress.
That drag event tells the app to open the Build side-drawer.
The Build drawer is not a new room; it is a tool chest that is bolted to the wall of the Plan room. The developer sees proposed code, accepts or edits, and pushes a branch.
	6.	Tests run and results come back.
When tests pass or fail, the Test agent pushes a new alert. Where does that alert appear?
It appears back in the foyer, Mission Control, because that foyer is the single place for urgent decisions.
Anybody can click the house icon (or press a keyboard shortcut) to jump from whatever room they are in back to the foyer, see the red or green dot, and open the evidence in the right-hand panel.

So:
	•	Mission Control is always the starting spot and the place you return to for time-critical events.
	•	Think, Define, Plan, Build, Validate are five focused rooms you step into to do sustained work on one artefact.
	•	Dragging a card or pressing a button is how you move an artefact—and yourself—from one room to the next.
	•	You never see all six areas at once, so the screen stays quiet; but they are always a single click or drag away, so the flow feels continuous.

If you picture the product this way—as one calm foyer plus five specialised rooms—the UI changes you saw in the mock-ups line up: first we stood in the foyer, then we went into Define and later into Plan. The furniture changed because each room serves a different purpose, but the house—and its address bar—never changed.
Here’s a fuller mock-up of the first screen that now shows both elements:
	•	Top navigation tabs – Think · Define · Plan · Build · Validate.
They sit like room labels across the width. Think is highlighted, because we are still in the foyer.
	•	Left sidebar – Projects.
Each project name has a dot—green, amber, or red—so you can tell health at a glance.
	•	Centre feed – Untriaged thoughts and live alerts.
The grey card “Offer users Dark Mode” sits under the heading Untriaged thoughts.
A mouse-pointer hovers over it to hint that you can drag it upward to the Define tab.
Below it are the yellow and red items that need decisions.
	•	Right panel – Conversation view.
The selected red alert is open, showing the code diff, a quick note, and the prompt box that begins “/fix…”

This completes the foyer view and makes it obvious how a product owner can grab a raw idea and drop it into the next stage. Let me know what you’d like adjusted or which screen we should visualise next.From “raw idea” to an executable plan — one continuous ride, no dead ends

1.  Dragging the grey card onto Define

When the product owner releases that grey card onto the Define tab, three visible things happen at once:
	1.	The centre column morphs from the alert feed into a tidy list called Ideas in Definition.
	2.	The right-hand panel swells to fill most of the screen. It is now a Product Brief editor.
	3.	A slim progress bar appears along the top edge that quietly tracks how complete the brief is (0 % → 100 %).

Nothing is hidden behind a modal. The owner is writing in place.

2.  Filling the Product Brief — what the owner actually sees

The editor is already pre-populated by the Capture agent. Each section contains a starter sentence that came from the LLM:
	•	Problem statement – a single sentence the model guessed from the Slack message.
	•	Success metrics – two empty bullet points.
	•	Risks – one generic risk (“may increase battery drain”) as a prompt.
	•	Competitive notes – a tiny table showing which rival apps have dark themes.
	•	User stories – the agent has drafted three “As a… I want… so that…” items, each with acceptance criteria.

The owner can overtype, delete, or add more sections. All the while the progress bar advances as mandatory parts are filled.

3.  Pressing Freeze spec — the handshake that shifts the idea into Plan

The moment the blue Freeze spec button is pressed, two events fire behind the scenes:
	•	an immutable “Spec version 1” snapshot is saved so future code can be compared against it;
	•	an event spec.frozen is broadcast.

That event is the only trigger the Planner agent needs. Within a second the screen slides—no hard navigation, just a sideways glide—into the Plan stage.

4.  Arriving in Plan — what the UI shows

Now the centre of the screen is a lane board with three columns: Ready, In progress, Done.

Every card you see came from the Planner agent. It read the user-stories section of the brief, split them into discrete tasks, and attached a suggested owner and an effort estimate. For “Offer users Dark Mode” you might see:
	•	Create theme palette – suggested owner: Priya
	•	Update header styling – suggested owner: Alex
	•	Write migration script – suggested owner: Sam
	•	Add unit tests – suggested owner: Dana
	•	Run accessibility review – suggested owner: QA-Bot

All of them sit in Ready, waiting for humans to pull them.

5.  Editing or extending the plan

The product owner can:
	•	drag a card to a different column to mark status;
	•	open a card and adjust scope or assignee;
	•	type “/split” inside a card to ask the Planner agent to break it into smaller tasks;
	•	press Add story to write an extra user story, which spawns new tasks when saved.

Any change to the plan is mirrored back into the Product Brief automatically, so the brief stays the single source of truth.

6.  Developers take over — smooth hand-off from Plan to Build

As soon as a developer drags “Create theme palette” into In progress a drawer slides out from the right:
	•	the top half shows the Figma palette the designer created;
	•	the lower half contains skeleton code the Build agent generated (variables, colour tokens, basic unit tests).

The developer can accept the code or overwrite it. On commit, the Build agent watches the branch and keeps the Product Brief status tracker up to date.

7.  Continuous loop back to the Brief
	•	When tests pass, the Test agent appends the run to the brief.
	•	When the feature is merged, the Validate agent marks the user stories as “Satisfied.”
	•	The progress bar across the top quietly ticks to 100 %.

At any moment the owner can tap the Define tab again and open the brief; it now reads like a complete case file—idea, spec, tasks, code links, test evidence, deployment note.

⸻

Why this matters: the owner never worries about “Where do I write a BRD?” or “When do I make user stories?” The Product Brief is the BRD, the PRD, and the user-story collection in one living document. The stage labels—Think, Define, Plan, Build, Validate—simply control which helpers are active and which furniture (list, editor, board, code drawer, test dashboard) is on the screen at that moment.Let me walk you through two things in ordinary, un-fancy English:
	1.	How the invisible plumbing really works once the card leaves the board—from GitHub branch to generated tests and back.
	2.	Whether it is wiser to rebuild every screen from scratch or to act as an air-traffic tower that merely watches the best-in-class tools you already have.

⸻

1 What actually happens after you drag a task to “In Progress”
	1.	A branch appears in GitHub without anyone typing a command.
When the developer drops “Create theme palette” into In Progress, the Build agent calls the GitHub API behind the scenes:
	•	creates a branch named feature/dark-mode-palette from main;
	•	pushes a single commit containing the skeleton CSS tokens and a placeholder test file;
	•	opens a draft pull request that links back to the task ID.
The change shows up in the normal GitHub UI, so engineers who live in GitHub stay in GitHub.
	2.	The developer codes as usual.
They git pull, hack away in VS Code, run npm test, push commits. Nothing exotic here. The only extra is a tiny commit-msg guideline: include the task ID somewhere ([SF-42]). That lets the Build agent keep track of who is touching what.
	3.	On every push GitHub Actions fires.
We ship a ready-made workflow file in the repo (.github/workflows/ci.yml). It:
	•	installs dependencies;
	•	runs the generated unit tests plus any existing tests;
	•	posts a status back to the pull request.
	4.	If coverage drops or tests fail the Test agent intervenes.
GitHub’s webhook tells the platform, “CI failed.” The Test agent:
	•	looks at the diff (git diff against main);
	•	identifies files that changed colour variables;
	•	asks an LLM for extra test cases that isolate those lines;
	•	commits a new file like __tests__/palette.darkmode.spec.js to the same branch and comments on the PR:
“I added three snapshot tests for the new grey scale. Please review.”
The human can accept, tweak, or discard those tests exactly like any other code review.
	5.	When the PR turns green and gets two approvals the Build agent merges.
The merge event triggers another webhook that marks the task card “Done,” adds a line in the Product Brief—“Palette merged at SHA abc123”—and bumps the progress bar.

Nothing in that sequence locks a developer into a special IDE or a proprietary CI. It is just a thin layer of helpers that sit on top of the exact GitHub flow people already know.

⸻

2 Full “walled garden” versus “smart aggregator”—which path makes sense?

The case for re-using every existing tool
	•	Tools like Linear, Figma, and GitHub have huge R-and-D budgets. Competing with their raw polish is a fool’s game.
	•	If you force a team to abandon habits they love, adoption dies.
	•	A connector can fetch data in real-time—Linear’s GraphQL, Figma’s REST, GitHub’s webhooks—so an agent can still reason over everything without yet another UI.

The case for a single, opinionated surface
	•	Context switching hurts deep work. Even if integrations are tight, hopping from browser tab to desktop app to chat thread costs mental energy.
	•	Overlapping metaphors breed confusion. Is a “ticket” the same thing in Linear, GitHub, and TestRail? If you own the data model you can make one clean vocabulary.
	•	Some personas (founders, PMs) want to live in a place that feels tailored to them, not in a forest of engineering tools.

A middle path that tends to work in practice
	1.	Keep the heavy tools.
Designers still draw in Figma, code lives in GitHub, power users can keep Linear if they adore it.
	2.	Ingest, don’t iframe.
Instead of stuffing an embedded Figma canvas into a sidebar, the Capture service copies the palette, frame name, and a PNG snapshot into the knowledge graph. If a designer updates the frame the webhook sends a diff that refreshes the snapshot inside the Build drawer. You see the latest truth, but you are not forced into Figma just to eyeball the colours.
	3.	Offer a minimalist surface only where context truly converges.
Mission Control is not there to replace each tool—it is there to answer the cross-tool questions the originals can’t:
	•	“Is Dark Mode blocked by design or by failing tests?”
	•	“Which spec lines are still uncovered by code?”
	•	“How many high-risk items are unassigned right now?”
	4.	Treat the “rooms” (Define, Plan, Build, Validate) as helpers, not mandatory editors.
If a PM already writes a brilliant PRD in Notion, they can paste the link into the Product Brief and press Import. The Define room then extracts the headings, user stories, and metrics and carries on. The PM never has to leave Notion if they prefer.
	5.	Design for gradual adoption.
Day 1 you might connect Slack and GitHub only. Day 30 you add the Build drawer because engineers ask for code suggestions. Day 90 you turn on automated test generation. There is no cliff.

In short
You do not have to rebuild every knob and dial of existing platforms. What you do need is a single place where all the moving parts know about each other so that an agent can reason end-to-end. Whether that place is a full new UI or a quiet console that only shouts when something is off-track is a product decision, not a technical necessity. Most teams end up with a hybrid: heavyweight craft remains in best-of-breed tools; lightweight cross-cutting insight lives in Software Factory.

⸻

Grey areas you flagged: how tests are “valid” and how agents avoid garbage
	•	The Test agent never auto-merges new tests; it opens a pull request like a junior teammate. Humans still review.
	•	Every LLM suggestion—code, test, or spec—runs through a rule-checker: compilable, passes ESLint, matches the coding-style config. If it fails, the suggestion is trashed and the agent posts a polite “I couldn’t produce a clean patch” note.
	•	Coverage improvement is measured by the CI workflow; if coverage does not rise, the merge gate remains closed until a human approves an override.
	•	Flagged risk (amber) or failing tests (red) bubble up to Mission Control without killing momentum in Plan or Build, so the flow stays visible but not blocked.

⸻

Should you go all-in or integrate first?

If your company already runs smoothly on Linear, Figma, and GitHub, start with connectors. Let the agents prove their worth purely as smart observers.

If you have no established process or you are a green-field startup, owning one clean interface from day one often reduces churn and onboarding time.

Either path can lead to the same end-state: humans do their craft in the tools they love, while Software Factory supplies the connective tissue, the AI muscle, and the single page where the story of an idea—problem → spec → plan → code → test → deployment—can be read without opening six tabs.

Let me know which path feels right for your team and we can sketch a concrete rollout plan.Below is a straight-talk evaluation of how Goose, JACoB, and a multi-LLM layer could sit inside Software Factory, drawn from their public docs and repos. I cover what each tool really does, why you might adopt it, where it fits in the pipeline we have been sketching—and where the sharp edges are.

⸻

1 Goose (Block)

What it is
Goose is an open-source “on-machine AI agent” delivered as a CLI or desktop app. You point it at a working folder, choose an LLM (Gemini, GPT-4o, Claude 3, etc.), and type natural-language tasks like “build me a tic-tac-toe game.” Goose plans, writes files, runs commands, and can be extended through “extensions” or external MCP servers.  ￼

Strengths
	•	Local execution – no code ever leaves the dev box unless you choose; useful for privacy-sensitive clients.
	•	Pluggable LLMs – config wizard lets you swap providers by editing one YAML or interactive menu.  ￼
	•	Extensions – e.g. Computer Controller gives Goose browser automation and file caching; you can add your own for internal tools.  ￼
	•	MCP (Model Context Protocol) – a structured way to expose custom tools and data to the LLM; handy if you want Goose to talk to your knowledge graph without big rewrites.

Weaknesses / friction
	•	No native GitHub flow – Goose writes to the local FS; you still have to git add/commit/push (or script it).
	•	Single-user focus – collaboration primitives (PR comments, review threads) are not first-class.
	•	Learning curve for extensions – each extra power (e.g. reading your graph) requires Go or Python glue.

Where it could fit
	•	Replace your “Build drawer” prototype: when a dev drags a card to In Progress, spawn a Goose session in a container bound to the repo, pre-seed it with an outline plan and the Figma palette, then let it make commits.
	•	Use Goose’s CLI from inside GitHub Actions for heavier refactors: the workflow could goose session --script plan.md and auto-push the result as a PR.

⸻

2 JACoB (Just Another Coding Bot)

What it is
A GitHub App + Figma plugin + node service that watches issues, reads linked designs, converts them to NextJS/Tailwind code, opens a pull request, and keeps chatting in the PR thread. It positions itself as an “AI teammate” rather than a generic agent.  ￼

Strengths
	•	Tight GitHub integration – branches, PRs, review comments, and status checks are all automated once you install the App and set the webhooks.  ￼
	•	Design-to-code pipeline – built-in Figma plugin parses frames and spits out React + Tailwind that compiles.
	•	Opinionated scaffolding – comes with NextJS, tRPC, Tailwind, and a database layer so it can run a staging preview out of the box.
	•	White-paper level docs & active Discord – richer guidance than most OSS agents.

Weaknesses / friction
	•	Narrow language & stack – TypeScript + NextJS only for now; if your backend is Go or Java you get zero lift.  ￼
	•	Heavy self-host setup – needs RabbitMQ, Postgres, smee proxy, and a custom GitHub App; lots of moving parts.
	•	LLM dependency through PortKey – the OSS version expects your own OpenAI key or a local Ollama; juggling multiple providers means more tweaking.

Where it could fit
	•	Treat it as a specialised Designer-to-Frontend lane inside Build. When a Figma link lands in a task, route that task to JACoB; its PR becomes one of the artefacts your knowledge graph tracks.
	•	Harvest its PR comments and diff summaries to enrich the “right-hand conversation panel” in Mission Control.

⸻

3 A practical multi-LLM layer

Because you have access to OpenAI, Anthropic, Gemini, and maybe others, add a thin broker service:

Step	Broker duty	Best model today
Specification expansion (turn Slack idea → full brief)	needs long context + structured JSON	Claude 3.5 Sonnet or GPT-4o
Task slicing (turn user stories → cards)	deterministic, small context	GPT-3.5-Turbo or Gemini Flash
Code synthesis & inline refactor	high reasoning + tool calls	GPT-4o or Goose-chosen model
Test generation from diff	pattern-matching + small context	Claude 3.5 Sonnet
Conversational chat in right panel	latency sensitive	Gemini Flash

The broker can be as small as: “look at token budget + temperature + latency target → pick a route.” Goose already supports multiple models; JACoB uses PortKey and virtual keys, so you can mirror the same routing table there.

⸻

4 Putting it together inside Software Factory
	1.	Keep the mission-control UI and knowledge graph. That is your differentiator—seeing the whole story in one map.
	2.	Swap in Goose for heavy local code generation.
	•	When a card hits In Progress, call your adapter that shells out to goose session --plan $json.
	•	When Goose finishes and pushes, listen for the webhook and close the loop in the graph.
	3.	Plug JACoB into the Figma-to-frontend path.
	•	If a task has a Figma URL, tag it design-to-code and let a GitHub Action label trigger JACoB’s workflow.
	•	The PR JACoB opens goes through the same Test agent and merge gate as human code.
	4.	Use the multi-LLM broker for all text-heavy tasks—brief completion, task splitting, root-cause analysis for failing tests, recap comments in PRs.
	5.	Fallback rule: if Goose or JACoB can’t handle a repo (e.g. Java monolith), fall back to a simpler “Create branch + ask GPT-4o for a patch” flow so the pipeline never stalls.

⸻

5 Should you adopt both?

Option	Benefit	Cost	Recommendation
Goose only	Local, language-agnostic, brings its own plan/execution engine	You must wire GitHub yourself; no design pipeline	Good starting point if your work is backend-heavy
JACoB only	Design-to-code polished, GitHub native	Only TS/NextJS, heavy infra	Worth it if your roadmap is UI-centric and in that stack
Both with broker	Best of each: Goose for generic code, JACoB for UI; unified LLM fan-out	Two integrations to maintain, broker to write	Fits the “all personas” vision; agents specialise, Factory orchestrates

Given your ambition to be an umbrella platform, running both makes sense: each becomes an agent that plugs into the wider event bus while your knowledge graph remains the single source of truth.

⸻

Concrete next step
	•	Prototype the broker: write a 200-line FastAPI service that proxies /chat and routes by a simple rule set.
	•	Wrap Goose in a GitHub Action (runs-on: ubuntu-latest, goose session --script .factory/plan.yaml) and track the resulting PR numbers in your graph.
	•	Install JACoB on a toy NextJS repo with a Figma design, let it open its first PR, and observe how the artefact chain appears in Mission Control.

If those three spikes land cleanly, expanding to real projects is an execution question, not a research question.Picture the right-hand column as a smart notebook that always opens to the exact page you need.

⸻

The moment you click a card in the middle column

Suppose the middle column shows an amber card that reads “Payment flow broke on Safari, approve rollback?” You click it once. Immediately, the right column does four things—quietly, almost at the same time—so that you can decide without hunting for information.
	1.	It rewinds the conversation.
At the top you see a short timeline:
	•	12:58 PM — automated test fails on Safari
	•	12:59 PM — Build agent suggests rollback
	•	1:00 PM — QA lead tags the card “needs decision”
This timeline is stitched together by reading Slack posts, GitHub comments and CI alerts that share the same ticket ID.
	2.	It pins the hard evidence next.
Right under the timeline you see a diff viewer. On the left is the line of code before the bug—chargeCustomer(amount)—and on the right is the new, broken line—chargeCustomer(amount, curr). A small red bar highlights the exact character difference. Beneath the diff there is a scrollable log excerpt from Cypress that shows the test failing only on Safari 17.
	3.	It adds the relevant plan and spec fragments.
Below the code you find the snippet of the Product Brief that defines the requirement:
“Charging a customer should always use the shop’s default currency, never the browser locale.”
Having that sentence right here spares you from opening the full spec.
	4.	It opens a prompt box primed with a live suggestion.
The prompt already contains /fix revert-to commit abc123? You can hit Enter to accept, or you can type something like “Add a unit test for multi-currency before rolling back.” Pressing Enter sends that instruction to the Build agent, which then opens a pull request or attaches a task—same as if you had done it all manually.

All of this appears in a single scrollable column. There are no nested tabs, no hidden modals.

⸻

A different example: a design tweak instead of a failing test

Imagine you click a yellow card that says “Spacing updated, review visual diffs?” The right column changes accordingly.
	•	The timeline now shows:
	•	9:02 AM — designer Sara updated Figma frame
	•	9:03 AM — Capture agent snapshots the frame and posts alert
	•	The evidence section displays a slider overlay: move the handle left-right to see the old and new button spacing. You do not need to open Figma just to eyeball it.
	•	The spec fragment reminds you of the padding guideline: “Primary buttons must have 16 px horizontal padding.”
	•	The prompt offers /approve spacing-change or you can type “Ask Sara: does this affect mobile layout?”

Again, no other tabs, no external tools—everything you need sits right there.

⸻

How the right column stays up-to-date
	•	Whenever someone adds a GitHub comment, posts to Slack, or re-runs a failing test, the Capture agent sends a small event. If the ticket you’re viewing is affected, the column refreshes automatically—no reload needed.
	•	If you move away from the ticket and come back later, the column shows only the new events since your last visit, highlighted in pale blue so you can scan what’s changed.

⸻

Why the design matters
	•	You never stare at an empty chat box. It always starts with a useful suggestion that you can accept or edit.
	•	You never lose context. The code, the spec, the design, the history—they all follow the ticket.
	•	You never wonder what to do next. The prompt is pre-filled with the most common next action, based on the ticket type and the project’s merge policy.

In short, the right column behaves like an eager junior teammate who has gathered every scrap of evidence, laid it out in neat order, and is already holding a pen over the “next step” box—waiting only for your word to proceed.



Below is one long-form story that shows exactly how a product owner named Anna and a staff engineer named Rahul will live inside Software Factory from the first spark of an idea to the moment the fix reaches production.
Every screen the two of them touch is described in plain English. Each invisible step is spelled out so Rahul—and any other engineer—knows why it happens and which tool or model does the lifting.

⸻

1 Monday morning – Anna in Mission Control

Anna opens the web app and lands on Mission Control.
A slim navigation bar across the top reads Think · Define · Plan · Build · Validate but the page that loads first is the foyer:
	•	On the left rail four project names glow green except “Payments,” which glows amber.
	•	The centre feed shows three alert cards. The top one says “Payment flow broke on Safari, approve rollback?”
	•	The right column is empty until she clicks a card, so for now she scans the feed.

What Rahul’s code is doing behind that calm screen
	•	A Capture service has been hoovering Slack, GitHub, and the CI pipeline all night, storing each artefact in a Neo4j knowledge graph.
	•	A tiny traffic-light service rolls up failures, unanswered questions and test flakiness into a single colour and pipes that colour to Mission Control every ten seconds.
	•	No AI is called yet—nothing to interpret, only raw status.

⸻

2 Tuesday – a marketing idea enters the room

Someone in Slack types: “Could we add Dark Mode? Users keep asking.”
The Capture service tags the sentence as a potential enhancement and drops a grey card called “Offer users Dark Mode” at the very top of the centre feed under the sub-heading Untriaged thoughts.

Anna drags that grey card upward onto the Define tab.
The screen doesn’t reload; it slides.
Now she sees the Product Brief editor on the right and an “Ideas in Definition” strip on the left.

How the AI steps in
	•	The Capture service asks a broker to pick the best model for expanding the one-liner into a structured brief.
	•	The broker chooses Claude 3.5 Sonnet because the text is long and must come back as valid JSON.
	•	Claude returns five sections: problem statement, success metrics, risks, competitive notes, and three draft user stories.
	•	The editor shows those sections with a thin blue progress bar that creeps from 0 % to 20 % because required headings now exist.

Anna tweaks two sentences, hits Freeze spec. The blue bar shoots to 100 %.

⸻

3 Instant slide into Plan

The UI glides sideways again.
Anna is now looking at a lane board with Ready, In Progress, Done.
Five cards sit in Ready:
	•	Create theme palette — avatar Priya
	•	Update header styling — avatar Alex
	•	Write migration script — avatar Sam
	•	Add unit tests — avatar Dana
	•	Run accessibility review — robot icon

Those cards appeared because the Planner agent parsed the frozen brief, chose verbs, estimated effort and assigned owners based on commit history.

Rahul’s implementation notes

Rahul wired the Planner like this:
	•	When it hears spec.frozen, it calls GPT-3.5-Turbo (cheap, fast) to break user stories into tasks.
	•	It then calls the GitHub API and pre-creates branches named after each task.
	•	It inserts the card data back into Neo4j and broadcasts tasks.created.

No proprietary board API is needed; it is just a React component reading the graph.

⸻

4 Wednesday – Priya picks up a card

Priya the front-end dev drags “Create theme palette” into In Progress.
The board stays on-screen, but a Build drawer slides out from the right.
The top of the drawer shows a row of greys and off-whites pulled straight from Figma; the bottom shows CSS custom-properties and a stub Jest test.

Anna, watching from Mission Control, sees Priya’s avatar appear on the card. Rahul, meanwhile, watches the plumbing.

What just happened under the hood
	1.	The front-end fires a POST /tasks/SF-42/start.
	2.	A small adapter Rahul wrote shells out to Goose with a plan file that bundles:
	•	the Figma palette PNG,
	•	the CSS token naming scheme,
	•	the branch name feature/dark-mode-palette.
	3.	Goose, running in a container, opens that branch, writes the skeleton code and the test file, then exits. Goose logs the commit SHAs. ￼ ￼
	4.	A GitHub webhook fires back to Software Factory: “new commits, new PR draft.”
	5.	The Build drawer listens to that webhook stream and refreshes live, so Priya sees her own code seconds later, ready for edits.

Goose can do this because it is LLM-agnostic and Rahul pointed it at GPT-4o for code quality. ￼ ￼

⸻

5 Design work joins in – JACoB’s turn

Alex the designer updates the master button frame in Figma.
Capture hears the Figma webhook, notices the frame is linked to the “Update header styling” task, and tags the task design-to-code.
A GitHub Action with the same label spins up JACoB. JACoB pulls the Figma frame, turns it into React + Tailwind, and opens a PR. ￼ ￼

Anna clicks the new PR notice in Mission Control; the right column shows:
	•	an image slider of old vs new header,
	•	JACoB’s code diff,
	•	a prompt pre-filled with “/approve header-change”.

She types “Looks good, merge after tests.”
The message lands as a GitHub review comment because Capture mirrors chat back to GitHub.

⸻

6 Thursday – Tests turn red

Dana pushes her unit tests. One fails on Safari.
GitHub Actions posts the failure.
The Test agent grabs the diff and asks Claude 3.5 to suggest three new snapshot tests.
It commits them on a branch ai/tests/darkmode-safari and opens a PR addressed to Dana.

The right column, still showing the failing card, refreshes on its own:
	•	new timeline entry: “AI added tests, review?”
	•	diff viewer shows the new snapshot files.
	•	prompt box autofills “/merge ai/tests after review”.

⸻

7 Friday – Everything passes, Validate kicks in

All tests turn green. Priya’s palette PR and JACoB’s header PR both meet the merge gate (two approvals, full green).
The Validate screen lights up automatically: a grid of test cases with green checks, a video replay of Cypress in Safari, and a single button Deploy to staging.
Anna clicks it. The Deployment agent uses the GitHub environment API and posts back the URL.

Mission Control now shows the Payments project dot turn green again.

⸻

8 What Rahul has running on the server by the end of the week
	•	A graph database that links every Slack idea, Figma frame, GitHub commit, and test run.
	•	A broker service that routes LLM calls: Claude for long structured output, GPT-3.5 for cheap task slicing, GPT-4o for heavy code, Gemini Flash for chat-like latency.
	•	Goose containers that spin up on demand for generic code generation, always linked to a branch and torn down after push.
	•	JACoB running as its own GitHub App, triggered only when a task owns a Figma frame.
	•	A front-end that shows only three persistent screens but morphs their furniture: Mission Control, Product Brief, Kanban board. Everything else is a drawer or a side-panel.

Rahul writes in his journal:

“I didn’t rebuild Linear or Figma or GitHub.
I let each tool stay where it shines.
My job was to listen to their webhooks, cram the data into the graph, and let agents—Goose, JACoB, Claude, GPT—pull just enough context to act.
The UI’s job is to keep Anna, Priya and Dana in one calm place while all that noise happens backstage.”

⸻

Why the system feels simple to Anna yet deep to Rahul
	•	Anna never sees a path longer than one click. Every screen answers her next question in the same spot: top for timeline, middle for evidence, bottom for “what do I do?”
	•	Rahul never kludges brittle if-else trees. Every new capability—new model, new agent—listens to the same event bus and writes back to the same graph.

The marriage of Goose for broad code work, JACoB for pixel-perfect design code, and a broker that chooses the best LLM for text and reasoning means Software Factory grows sharper every sprint without becoming a maze of half-integrated SaaS tabs.

Below is a beginner-friendly walk-through of the “design-to-code” flow.
For every step you will see:
	•	What happens in plain English
	•	Why that step exists
	•	A few lines of very short pseudocode so you can picture the implementation.

I include only the essential jargon and explain it the moment it appears.

⸻

1 Designer edits a frame in Figma

What happens

Alex changes the padding on the primary button in Figma and hits Save.
Figma immediately sends a tiny HTTP message—called a webhook—to an address you registered.

Jargon: Webhook
A webhook is just an automatic POST request that a service (here, Figma) fires whenever something interesting happens.

Pseudocode (Node + Express)

// server.js
app.post('/figma-webhook', async (req, res) => {
  const frameId   = req.body.node.id      // "123:456"
  const figmaFile = req.body.file_key     // "AbCdEf"
  queue.add({ frameId, figmaFile })       // hand off for later
  res.status(200).end()
})


⸻

2 Link the Figma frame to the Software-Factory task

What happens

Your service looks up which task owns that frame. You stored that connection earlier when the planner created tasks.

const taskId = graph.findTaskByFigmaNode(frameId)  // returns "SF-77"

If the task exists, you call the GitHub API and add a label named design-to-code to the corresponding GitHub issue or pull request.

Jargon: Label
A coloured tag you stick on an issue or PR so automations (and humans) can filter items.

await github.issues.addLabels({
  owner, repo, issue_number: taskId,
  labels: ['design-to-code']
})


⸻

3 A GitHub Action starts automatically

What happens

On that repository you have a workflow file named .github/workflows/design-to-code.yml.
It says: “Whenever an issue receives the label design-to-code, run this job.”

Jargon: GitHub Action
A pre-built CI system in GitHub. You describe when it should run and what steps to execute in YAML.

workflow snippet

on:
  issues:
    types: [labeled]

jobs:
  jacob_job:
    if: github.event.label.name == 'design-to-code'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4   # pull down the code
      - name: Run JACoB
        run: |
          jacob \
            --figma-file ${{ secrets.FIGMA_FILE }} \
            --figma-node ${{ github.event.issue.body }} \
            --openai-key ${{ secrets.OPENAI_KEY }}
      - name: Create PR
        run: gh pr create --fill

jacob is your CLI wrapper around the open-source JACoB project.
It takes the Figma info, talks to an LLM, writes React + Tailwind files into the repo, and the next step opens a pull request (PR).

Jargon: Pull request (PR)
A request to merge a branch back into main code. It shows a list of commits and file differences (diffs) so teammates can review.

⸻

4 JACoB does the heavy lifting

What happens inside the jacob command
	1.	Downloads the Figma frame as SVG or PNG.
	2.	Prompts GPT-4o (or the model you configure) with “Turn this frame into React + Tailwind.”
	3.	Writes the component file, CSS file, and a storybook story.
	4.	Commits those files on a new branch jacob/SF-77/header-styling.

You do not have to write code for these four steps; they are baked into the JACoB repo. You only pass in the keys.

⸻

5 GitHub fires a second webhook: “PR opened”

What happens

GitHub sends a POST to your Capture service describing the new PR. Capture:
	1.	Reads the PR title to find SF-77.
	2.	Creates a PR node in the knowledge graph and links it to the task.
	3.	Broadcasts an event pr.opened.

Your front-end is listening. It adds a yellow card in Mission Control saying “Header styling PR open”.

⸻

6 Anna sees the card and clicks it

What happens on screen (right column)
	1.	Image slider – Capture already downloaded the “before” and “after” PNGs during the Figma and PR events, so it renders a small slider widget.
	2.	Code diff – The GitHub diff API returns raw patch text; a browser component renders the coloured add/remove lines.
	3.	Prompt box – The UI prefills /approve header-change because:
	•	the checklist shows no risks,
	•	tests are still pending,
	•	the merge policy is “auto-merge after green.”

Anna types “Looks good, merge after tests” and presses Enter.

⸻

7 Your service mirrors Anna’s comment back to GitHub

await github.issues.createComment({
  issue_number: prNumber,
  owner, repo,
  body: "Looks good, merge after tests"
})

Now the conversation is visible in both places: the right column and the GitHub PR thread.

⸻

8 CI tests finish; the PR auto-merges

A GitHub Action named ci.yml finishes with ✓.
Another Action, auto-merge.yml, checks:
	•	✓ tests pass
	•	✓ at least one human approval (Anna’s)
	•	✓ no remaining “changes requested”

It then merges the branch. GitHub emits a pull_request.closed webhook. Capture marks the task “Done,” the dot in Mission Control flips from yellow to green, and your job is finished.

⸻

9 What to build first if you are new
	1.	Figma → Node server
Set up the webhook route and just log the JSON to console.
	2.	Node → GitHub label
Use Octokit (GitHub’s SDK) to add a label to a test issue.
	3.	GitHub Action
Create a workflow that runs echo "Hello" when that label appears.
	4.	Replace echo with jacob command
Point it at a dummy Figma file, commit to a branch, open a PR.
	5.	Capture webhooks
Have another small Node route that listens to pull_request events and logs them.
	6.	Simple front-end
Render the list of logged PRs on a webpage; clicking one shows the diff text.

Each step gives you a visible payoff and moves you closer to the full flow without drowning you in complexity.

⸻

Recap in one sentence

The whole dance is just webhooks passing messages: Figma → your server → GitHub label → GitHub Action → JACoB → Pull Request → Mission Control UI—each hop adding the minimum glue code you saw in the snippets above.